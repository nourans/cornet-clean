{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d7b1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C&W - Running on full dataset: 3923 total images\n",
    "# Put this into a Jupyter notebook cell\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from cornet import cornet_s\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "# -------------------------\n",
    "# Dataset: ALL images = test set\n",
    "# -------------------------\n",
    "class AllImagesAsTestDataset(Dataset):\n",
    "    \"\"\"Wraps a list of (image_path, label) - used as the entire test set (no split).\"\"\"\n",
    "    def __init__(self, samples: List[Tuple[str, int]], transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p, label = self.samples[idx]\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "def build_all_test_samples(root: str, sort_filenames: bool = True) -> Tuple[List[Tuple[str,int]], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Walks `root` and builds a list of (image_path, label) for *all* files in each class folder.\n",
    "    Label assigned by sorted folder order. Returns samples and class_map.\n",
    "    \"\"\"\n",
    "    root_p = Path(root)\n",
    "    assert root_p.exists(), f\"Data root not found: {root}\"\n",
    "    class_dirs = sorted([d for d in root_p.iterdir() if d.is_dir()])\n",
    "    samples = []\n",
    "    class_map = {}\n",
    "    for label, cls in enumerate(class_dirs):\n",
    "        imgs = [p for p in cls.iterdir() if p.is_file()]\n",
    "        if sort_filenames:\n",
    "            imgs = sorted(imgs)\n",
    "        for p in imgs:\n",
    "            samples.append((str(p), label))\n",
    "        class_map[label] = cls.name\n",
    "    return samples, class_map\n",
    "\n",
    "# -------------------------\n",
    "# Carlini & Wagner (L2, untargeted)\n",
    "# -------------------------\n",
    "def cw_l2_attack(\n",
    "    model,\n",
    "    images,\n",
    "    labels,\n",
    "    device,\n",
    "    c=1e-3,\n",
    "    kappa=0.0,\n",
    "    steps=100,\n",
    "    lr=1e-2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Untargeted Carlini & Wagner L2 attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Inverse tanh-space transform\n",
    "    eps = 1e-6\n",
    "    x = torch.clamp(images, 0, 1)\n",
    "    w = torch.atanh((x * 2 - 1) * (1 - eps))\n",
    "    w = w.detach().clone().requires_grad_(True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([w], lr=lr)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        adv = torch.tanh(w) / 2 + 0.5\n",
    "\n",
    "        logits = model(adv)\n",
    "\n",
    "        one_hot = torch.eye(logits.size(1), device=device)[labels]\n",
    "        real = torch.sum(one_hot * logits, dim=1)\n",
    "        other = torch.max((1 - one_hot) * logits - one_hot * 1e4, dim=1)[0]\n",
    "\n",
    "        f = torch.clamp(real - other + kappa, min=0)\n",
    "\n",
    "        l2 = torch.sum((adv - images) ** 2, dim=(1, 2, 3))\n",
    "        loss = torch.mean(l2 + c * f)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    adv = torch.tanh(w) / 2 + 0.5\n",
    "    return adv.detach()\n",
    "\n",
    "def l2_norms(x_adv, x):\n",
    "    return torch.norm((x_adv - x).view(x.size(0), -1), dim=1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# FGSM helper (normalized-space)\n",
    "# -------------------------\n",
    "def fgsm_perturb_from_grad(x: torch.Tensor, grad: torch.Tensor, epsilon: float) -> torch.Tensor:\n",
    "    \"\"\"Return perturbed inputs in normalized input space using gradient sign.\"\"\"\n",
    "    return torch.clamp(x + epsilon * grad.sign(), -10.0, 10.0).detach()\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation (clean + FGSM)\n",
    "# -------------------------\n",
    "def evaluate_clean(model: nn.Module, loader: DataLoader, device: torch.device) -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return (correct / total) if total > 0 else 0.0\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation (C&W cross-model)\n",
    "# -------------------------\n",
    "\n",
    "def evaluate_cw_l2_cross(\n",
    "    generating_model,\n",
    "    predicting_model,\n",
    "    loader,\n",
    "    device,\n",
    "    l2_budgets=(0.5, 1.0, 2.0, 4.0, 8.0),\n",
    "    cw_params=None,\n",
    "):\n",
    "    if cw_params is None:\n",
    "        cw_params = dict(c=1e-3, kappa=0.0, steps=100, lr=1e-2)\n",
    "\n",
    "    correct = {b: 0 for b in l2_budgets}\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        adv = cw_l2_attack(\n",
    "            generating_model,\n",
    "            images,\n",
    "            labels,\n",
    "            device=device,\n",
    "            **cw_params,\n",
    "        )\n",
    "\n",
    "        norms = l2_norms(adv, images)\n",
    "\n",
    "        logits = predicting_model(adv)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        for b in l2_budgets:\n",
    "            mask = norms <= b\n",
    "            correct[b] += (preds[mask] == labels[mask]).sum().item()\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = {b: correct[b] / total for b in l2_budgets}\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Pretrained model loaders\n",
    "# -------------------------\n",
    "def load_pretrained_alexnet(num_classes_expected: int, device: torch.device):\n",
    "    model = models.alexnet(pretrained=True)\n",
    "    if num_classes_expected != 1000:\n",
    "        # Replace final layer to match labels (user dataset labels)\n",
    "        in_feats = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_feats, num_classes_expected)\n",
    "        # note: the new head is random init (we're not training it here)\n",
    "        print(f\"[warning] AlexNet: dataset has {num_classes_expected} classes != 1000; final layer replaced (random init).\")\n",
    "    return model.to(device)\n",
    "\n",
    "def load_pretrained_vgg16(num_classes_expected: int, device: torch.device):\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    if num_classes_expected != 1000:\n",
    "        in_feats = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_feats, num_classes_expected)\n",
    "        print(f\"[warning] VGG16: dataset has {num_classes_expected} classes != 1000; final layer replaced (random init).\")\n",
    "    return model.to(device)\n",
    "\n",
    "def load_pretrained_cornet(num_classes_expected: int, device: torch.device):\n",
    "    # Load CORnet-S (ImageNet-pretrained)\n",
    "    model = cornet_s(pretrained=True)\n",
    "\n",
    "    # CORnet-S classifier is `decoder`\n",
    "    if num_classes_expected != 1000:\n",
    "        in_feats = model.decoder.in_features\n",
    "        model.decoder = nn.Linear(in_feats, num_classes_expected)\n",
    "        print(\n",
    "            f\"[warning] CORnet-S: dataset has {num_classes_expected} classes != 1000; \"\n",
    "            \"final layer replaced (random init).\"\n",
    "        )\n",
    "    return model.to(device)\n",
    "# -------------------------\n",
    "# Main runner: ALL images are test set\n",
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "565b8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pretrained_on_all_test(\n",
    "    data_dir: str,\n",
    "    gen_model_name: str,\n",
    "    pred_model_name: str,\n",
    "    batch_size: int = 128,\n",
    "    num_workers: int = 4,\n",
    "    device: str | None = None,\n",
    "    c: float = 1e-3,\n",
    "    kappa: float = 0.0,\n",
    "    steps: int = 100,\n",
    "    lr: float = 5e-3,\n",
    "):\n",
    "    device = torch.device(device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    print(f\"[eval_pretrained_on_all_test] using device: {device}\")\n",
    "\n",
    "    samples, class_map = build_all_test_samples(data_dir)\n",
    "    n_classes = len(class_map)\n",
    "    print(f\"Found {n_classes} classes and {len(samples)} total images (all used as test samples).\")\n",
    "\n",
    "    transform = T.Compose([\n",
    "        T.Resize(256),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "    test_ds = AllImagesAsTestDataset(samples, transform=transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # ---- Load models ----\n",
    "    if gen_model_name == \"alexnet\":\n",
    "        gen_model = load_pretrained_alexnet(n_classes, device)\n",
    "    elif gen_model_name in (\"cornet\", \"cornet_s\"):\n",
    "        gen_model = load_pretrained_cornet(n_classes, device)\n",
    "    elif gen_model_name == \"vgg16\":\n",
    "        gen_model = load_pretrained_vgg16(n_classes, device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown generating model {gen_model_name}\")\n",
    "\n",
    "    if pred_model_name == \"alexnet\":\n",
    "        pred_model = load_pretrained_alexnet(n_classes, device)\n",
    "    elif pred_model_name in (\"cornet\", \"cornet_s\"):\n",
    "        pred_model = load_pretrained_cornet(n_classes, device)\n",
    "    elif pred_model_name == \"vgg16\":\n",
    "        pred_model = load_pretrained_vgg16(n_classes, device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown predicting model {pred_model_name}\")\n",
    "\n",
    "    gen_model.eval()\n",
    "    pred_model.eval()\n",
    "\n",
    "    # ---- Clean accuracies ----\n",
    "    clean_acc = evaluate_clean(gen_model, test_loader, device)\n",
    "    print(f\"{gen_model_name} clean accuracy: {clean_acc*100:.2f}%\")\n",
    "\n",
    "    clean_acc = evaluate_clean(pred_model, test_loader, device)\n",
    "    print(f\"{pred_model_name} clean accuracy: {clean_acc*100:.2f}%\")\n",
    "\n",
    "    # ---- C&W cross-model attack ----\n",
    "    adv_acc = evaluate_cw_l2_cross(\n",
    "        generating_model=gen_model,\n",
    "        predicting_model=pred_model,\n",
    "        loader=test_loader,\n",
    "        device=device,\n",
    "        l2_budgets=[0.5, 1.0, 2.0, 4.0, 8.0],\n",
    "        cw_params=dict(\n",
    "            c=1e-3,\n",
    "            kappa=0.0,\n",
    "            steps=100,\n",
    "            lr=1e-2,\n",
    "        )\n",
    "    )   \n",
    "\n",
    "\n",
    "    for b, acc in adv_acc.items():\n",
    "        print(f\"{gen_model_name} generates & {pred_model_name} predicts | L2 ≤ {b} | acc={acc*100:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        gen_model_name: {\n",
    "            \"clean_acc\": clean_acc,\n",
    "            \"cross\": {pred_model_name: adv_acc},\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c8f1cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALEX generates --- VGG16 predicts\n",
      "\n",
      "[eval_pretrained_on_all_test] using device: cuda:0\n",
      "Found 1000 classes and 3923 total images (all used as test samples).\n",
      "alexnet clean accuracy: 56.31%\n",
      "vgg16 clean accuracy: 70.41%\n",
      "alexnet generates & vgg16 predicts | L2 ≤ 0.5 | acc=0.00%\n",
      "alexnet generates & vgg16 predicts | L2 ≤ 1.0 | acc=0.00%\n",
      "alexnet generates & vgg16 predicts | L2 ≤ 2.0 | acc=0.00%\n",
      "alexnet generates & vgg16 predicts | L2 ≤ 4.0 | acc=0.00%\n",
      "alexnet generates & vgg16 predicts | L2 ≤ 8.0 | acc=0.00%\n",
      "{'alexnet': {'clean_acc': 0.7040530206474637, 'cross': {'vgg16': {0.5: 0.0, 1.0: 0.0, 2.0: 0.0, 4.0: 0.0, 8.0: 0.0}}}}\n"
     ]
    }
   ],
   "source": [
    "print(\"ALEX generates --- VGG16 predicts\\n\")\n",
    "\n",
    "results = eval_pretrained_on_all_test(\n",
    "    \"val/val/\",\n",
    "    gen_model_name=\"alexnet\",\n",
    "    pred_model_name=\"vgg16\",\n",
    "    batch_size=8,\n",
    "    device=\"cuda:0\",\n",
    "    steps=100,\n",
    "    c=1e-3,\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9894bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condavenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
